{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e5cc4a2-443e-4563-b10f-82be25ce25bb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%restart_python\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%pip install fpdf\n",
    "%pip install reportlab\n",
    "%pip install --upgrade crewai\n",
    "%pip install --upgrade \"mlflow[databricks]>=3.1\" crewai\n",
    "# Enables autoreload; learn more at https://docs.databricks.com/en/files/workspace-modules.html#autoreload-for-python-modules\n",
    "# To disable autoreload; run %autoreload 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4e321424-abb7-43f9-b213-6c184d7f05dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# List installed packages\n",
    "%pip freeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ae89453-fe3f-40c2-b215-7fdf8442d76b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import sys\n",
    "import os\n",
    "from typing import Dict\n",
    "import seaborn as sns\n",
    "import base64\n",
    "import shutil\n",
    "\n",
    "def analyze_workflow(results):\n",
    "    \n",
    "    print(f\"\\nSimulation complete!\")\n",
    "    print(f\"   Run ID: {results['run_id']}\")\n",
    "    print(f\"   Final Titer: {results['final_titer']:.2f} mg/mL\")\n",
    "\n",
    "    # output simulation visualizations\n",
    "    fig = visualize_run(results)\n",
    "\n",
    "    # query telemetry data (temporal data from simulation log)\n",
    "    run_id = results['run_id']\n",
    "    telemetry_df = workflow.data_lake.get_run_telemetry(spark, run_id, is_observed=True)\n",
    "\n",
    "    print(f\"Telemetry Data Shape: {telemetry_df.shape}\")\n",
    "    print(f\"\\nSignals captured: {telemetry_df['signal_name'].unique()}\")\n",
    "    print(f\"Time range: {telemetry_df['time_h'].min():.1f} - {telemetry_df['time_h'].max():.1f} hours\")\n",
    "\n",
    "    # telemetry sample...\n",
    "    display(telemetry_df.head(10))\n",
    "    anomalies_df = workflow.data_lake.get_anomalies(spark, run_id, only_detected=True)\n",
    "\n",
    "    print(f\"Anomalies Detected: {len(anomalies_df)}\")\n",
    "    if not anomalies_df.empty:\n",
    "        print(\"\\nAnomalies by signal:\")\n",
    "        print(anomalies_df.groupby('signal_name').size())\n",
    "        print(\"\\nAnomalies by method:\")\n",
    "        print(anomalies_df.groupby('method').size())\n",
    "        \n",
    "        display(anomalies_df.head(10))\n",
    "    else:\n",
    "        print(\"No anomalies detected in this run\")\n",
    "\n",
    "    return fig\n",
    "\n",
    "def create_download_link(file_path, link_text=\"Download PDF\"):\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        data = f.read()\n",
    "    b64 = base64.b64encode(data).decode()\n",
    "    href = f'<a href=\"data:application/pdf;base64,{b64}\" download=\"{os.path.basename(file_path)}\">{link_text}</a>'\n",
    "    from IPython.display import display, HTML\n",
    "    display(HTML(href))\n",
    "\n",
    "# setup\n",
    "PROJECT_ROOT = '/Workspace/Repos/synthetic-twin/synthetic_twin'\n",
    "MODULES_PATH = os.path.join(PROJECT_ROOT, 'modules')\n",
    "if MODULES_PATH not in sys.path:\n",
    "    sys.path.insert(0, MODULES_PATH)\n",
    "\n",
    "# import modules\n",
    "from config import (SIMULATION_PARAMS, INITIAL_STATE, KINETIC_PARAMS, \n",
    "                    REACTOR_PARAMS, SENSOR_PARAMS, FAULT_TEMPLATES, SCENARIOS, SCORING_CONFIG)\n",
    "from models import BioreactorSimulation, FaultManager\n",
    "from anomaly_detection import (AnomalyDetectionEngine, create_default_bioreactor_config)\n",
    "from data_lake import BioreactorDataLake\n",
    "from run_simulation_workflow import SABRWorkflow, visualize_run\n",
    "from reporting import BioreactorPDFReport\n",
    "print(\"All modules imported successfully!\")\n",
    "\n",
    "# configuration\n",
    "config = {\n",
    "    'SIMULATION_PARAMS': SIMULATION_PARAMS,\n",
    "    'INITIAL_STATE': INITIAL_STATE,\n",
    "    'KINETIC_PARAMS': KINETIC_PARAMS,\n",
    "    'REACTOR_PARAMS': REACTOR_PARAMS,\n",
    "    'SENSOR_PARAMS': SENSOR_PARAMS,\n",
    "    'FAULT_TEMPLATES': FAULT_TEMPLATES\n",
    "    }\n",
    "\n",
    "# initialize workflow\n",
    "workflow = SABRWorkflow(spark=spark,config_dict=config,\n",
    "                            enable_agent=True,enable_anomaly_detection=True)\n",
    "\n",
    "# test multiple scenarios with replicates\n",
    "scenarios_to_test = ['standard', 'overfeed', 'DO_drop', 'contamination']\n",
    "\n",
    "print(\"Running batch scenario analysis...\")\n",
    "print(f\"   Scenarios: {scenarios_to_test}\")\n",
    "print(f\"   Replicates per scenario: 1\")\n",
    "print(f\"   Total runs: {len(scenarios_to_test) * 3}\")\n",
    "print(\"\\nThis may take several minutes...\\n\")\n",
    "\n",
    "reporter = BioreactorPDFReport()\n",
    "# create a timestamped output folder\n",
    "timestamp_str = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "output_folder = f\"/tmp/sabr_runs_{timestamp_str}\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "all_results = {}\n",
    "\n",
    "for scenario in scenarios_to_test:\n",
    "    workflow.inject_scenario_faults(scenario=scenario)\n",
    "    for rep in range(1, 2):  # 1 replicates\n",
    "        print(f\"\\nRunning simulation: {scenario}, replicate {rep}\")\n",
    "        results = workflow.run_with_monitoring(base_feed_rate=0.1, save_to_lake=True)\n",
    "        fig = analyze_workflow(results)\n",
    "        \n",
    "        run_timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        run_name = f\"{scenario}_rep{rep}_{run_timestamp}\"\n",
    "        \n",
    "        # store results in dictionary\n",
    "        all_results[run_name] = results\n",
    "        \n",
    "        # save CSV summary\n",
    "        csv_path = os.path.join(output_folder, f\"{run_name}_summary.csv\")\n",
    "        pd.DataFrame([{\n",
    "            'run_id': results['run_id'],\n",
    "            'scenario': scenario,\n",
    "            'replicate': rep,\n",
    "            'final_titer_g_L': float(results['final_titer']),\n",
    "            'final_biomass_g_L': float(results['final_biomass']),\n",
    "            'total_anomalies': int(results['num_anomalies']),\n",
    "            'success': bool(results['final_titer'] > 5.0),\n",
    "            'timestamp': run_timestamp,\n",
    "            'agent_explain': str(results['agent_explain'])\n",
    "        }]).to_csv(csv_path, index=False)\n",
    "        print(f\"Summary CSV saved to {csv_path}\")\n",
    "\n",
    "        pdf_file = reporter.generate_summary_pdf(\n",
    "            results=results,\n",
    "            telemetry_df=pd.DataFrame(results['observed_history']),\n",
    "            ai_summary=str(results['agent_explain']),\n",
    "            faults=[FAULT_TEMPLATES[scenario]],\n",
    "            param_config=config,\n",
    "            figures=[fig]\n",
    "        )\n",
    "        pdf_path = os.path.join(output_folder, f\"{run_name}_report.pdf\")\n",
    "        shutil.move(pdf_file, pdf_path)\n",
    "        print(f\"PDF report saved to {pdf_path}\")\n",
    "        create_download_link(pdf_path)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "synthetic_bioreactor",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
