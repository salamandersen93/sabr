{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e5cc4a2-443e-4563-b10f-82be25ce25bb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%restart_python\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "#%pip install --upgrade crewai\n",
    "#%pip install --upgrade \"mlflow[databricks]>=3.1\" crewai\n",
    "# Enables autoreload; learn more at https://docs.databricks.com/en/files/workspace-modules.html#autoreload-for-python-modules\n",
    "# To disable autoreload; run %autoreload 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ae89453-fe3f-40c2-b215-7fdf8442d76b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import sys\n",
    "import os\n",
    "from typing import Dict\n",
    "import seaborn as sns\n",
    "\n",
    "def analyze_workflow(results):\n",
    "    \n",
    "    print(f\"\\nSimulation complete!\")\n",
    "    print(f\"   Run ID: {results['run_id']}\")\n",
    "    print(f\"   Final Titer: {results['final_titer']:.2f} mg/mL\")\n",
    "\n",
    "    # output simulation visualizations\n",
    "    visualize_run(results)\n",
    "\n",
    "    # query telemetry data (temporal data from simulation log)\n",
    "    run_id = results['run_id']\n",
    "    telemetry_df = workflow.data_lake.get_run_telemetry(spark, run_id, is_observed=True)\n",
    "\n",
    "    print(f\"Telemetry Data Shape: {telemetry_df.shape}\")\n",
    "    print(f\"\\nSignals captured: {telemetry_df['signal_name'].unique()}\")\n",
    "    print(f\"Time range: {telemetry_df['time_h'].min():.1f} - {telemetry_df['time_h'].max():.1f} hours\")\n",
    "\n",
    "    # telemetry sample...\n",
    "    display(telemetry_df.head(10))\n",
    "    anomalies_df = workflow.data_lake.get_anomalies(spark, run_id, only_detected=True)\n",
    "\n",
    "    print(f\"Anomalies Detected: {len(anomalies_df)}\")\n",
    "\n",
    "    if not anomalies_df.empty:\n",
    "        print(\"\\nAnomalies by signal:\")\n",
    "        print(anomalies_df.groupby('signal_name').size())\n",
    "        \n",
    "        print(\"\\nAnomalies by method:\")\n",
    "        print(anomalies_df.groupby('method').size())\n",
    "        \n",
    "        display(anomalies_df.head(10))\n",
    "    else:\n",
    "        print(\"No anomalies detected in this run\")\n",
    "\n",
    "# setup\n",
    "PROJECT_ROOT = '/Workspace/Repos/synthetic-twin/synthetic_twin'\n",
    "MODULES_PATH = os.path.join(PROJECT_ROOT, 'modules')\n",
    "if MODULES_PATH not in sys.path:\n",
    "    sys.path.insert(0, MODULES_PATH)\n",
    "\n",
    "# import modules\n",
    "from config import (SIMULATION_PARAMS, INITIAL_STATE, KINETIC_PARAMS, \n",
    "                    REACTOR_PARAMS, SENSOR_PARAMS, FAULT_TEMPLATES, SCENARIOS, SCORING_CONFIG)\n",
    "\n",
    "from models import BioreactorSimulation, FaultManager\n",
    "from anomaly_detection import (AnomalyDetectionEngine, create_default_bioreactor_config)\n",
    "from data_lake import BioreactorDataLake\n",
    "from run_simulation_workflow import BioPilotWorkflow, visualize_run\n",
    "print(\"All modules imported successfully!\")\n",
    "\n",
    "# configuration\n",
    "config = {\n",
    "    'SIMULATION_PARAMS': SIMULATION_PARAMS,\n",
    "    'INITIAL_STATE': INITIAL_STATE,\n",
    "    'KINETIC_PARAMS': KINETIC_PARAMS,\n",
    "    'REACTOR_PARAMS': REACTOR_PARAMS,\n",
    "    'SENSOR_PARAMS': SENSOR_PARAMS,\n",
    "    'FAULT_TEMPLATES': FAULT_TEMPLATES\n",
    "    }\n",
    "\n",
    "# initialize workflow\n",
    "workflow = BioPilotWorkflow(spark=spark,config_dict=config,\n",
    "                            enable_agent=True,enable_anomaly_detection=True)\n",
    "\n",
    "# sample workflow:\n",
    "# injection of a fault scenario example - overfeeding @ 20h\n",
    "print(\"Injecting 'overfeed' fault at t=20h...\")\n",
    "workflow.inject_scenario_faults(scenario=\"overfeed\")\n",
    "\n",
    "# initiate simulation with a default base feed rate\n",
    "# TODO: make base feed rate customizable/dynamic\n",
    "print(\"\\nRunning simulation...\")\n",
    "results = workflow.run_with_monitoring(base_feed_rate=0.1,save_to_lake=True)\n",
    "analyze_workflow(results)\n",
    "\n",
    "# test multiple scenarios with replicates\n",
    "scenarios_to_test = ['standard', 'overfeed', 'DO_drop', 'contamination']\n",
    "\n",
    "print(\"Running batch scenario analysis...\")\n",
    "print(f\"   Scenarios: {scenarios_to_test}\")\n",
    "print(f\"   Replicates per scenario: 3\")\n",
    "print(f\"   Total runs: {len(scenarios_to_test) * 3}\")\n",
    "print(\"\\nThis may take several minutes...\\n\")\n",
    "\n",
    "for scenario in scenarios_to_test:\n",
    "    workflow.inject_scenario_faults(scenario=scenario)\n",
    "    print(f\"\\nRunning simulation for {scenario}...\")\n",
    "    results = workflow.run_with_monitoring(base_feed_rate=0.1,save_to_lake=True)\n",
    "    analyze_workflow(results)\n",
    "\n",
    "# test sensitivity to different growth rates (mu_max) (parameter sweep)\n",
    "mu_max_values = [0.03, 0.04, 0.05, 0.06]\n",
    "sweep_results = []\n",
    "\n",
    "print(\"Running parameter sweep: mu_max\")\n",
    "print(f\"   Testing values: {mu_max_values}\")\n",
    "\n",
    "for mu_max in mu_max_values:\n",
    "    print(f\"\\n   Testing mu_max = {mu_max}...\")\n",
    "    \n",
    "    # modify config\n",
    "    custom_config = config.copy()\n",
    "    custom_config['KINETIC_PARAMS'] = KINETIC_PARAMS.copy()\n",
    "    custom_config['KINETIC_PARAMS']['mu_max'] = mu_max\n",
    "    \n",
    "    # run simulation\n",
    "    workflow_sweep = BioPilotWorkflow(spark=spark,config_dict=custom_config,\n",
    "                                      enable_agent=True,enable_anomaly_detection=True)\n",
    "    \n",
    "    results_sweep = workflow_sweep.run_with_monitoring(base_feed_rate=0.1,save_to_lake=True)\n",
    "    sweep_results.append({'mu_max': mu_max,\n",
    "                          'final_titer': results_sweep['final_titer'],\n",
    "                          'final_biomass': results_sweep['final_biomass']})\n",
    "\n",
    "# analyze sweep results\n",
    "sweep_df = pd.DataFrame(sweep_results)\n",
    "print(\"\\nParameter sweep complete...\")\n",
    "display(sweep_df)\n",
    "\n",
    "# output sweep result visualizations\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "ax1.plot(sweep_df['mu_max'], sweep_df['final_titer'], 'o-', linewidth=2, markersize=8)\n",
    "ax1.set_xlabel('mu_max [1/h]', fontsize=11)\n",
    "ax1.set_ylabel('Final Titer [mg/mL]', fontsize=11)\n",
    "ax1.set_title('Titer vs Growth Rate', fontweight='bold')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "ax2.plot(sweep_df['mu_max'], sweep_df['final_biomass'], 'o-', linewidth=2, markersize=8, color='green')\n",
    "ax2.set_xlabel('mu_max [1/h]', fontsize=11)\n",
    "ax2.set_ylabel('Final Biomass [g/L]', fontsize=11)\n",
    "ax2.set_title('Biomass vs Growth Rate', fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# generate run summary\n",
    "run_summary = workflow.data_lake.get_run_summary(spark, results['run_id'])\n",
    "output_path = f\"/tmp/biopilot_run_{results['run_id']}_summary.csv\"\n",
    "\n",
    "summary_export = pd.DataFrame([{\n",
    "    'run_id': str(results['run_id']),\n",
    "    'scenario': 'overfeed',\n",
    "    'final_titer_mg_mL': float(results['final_titer']),\n",
    "    'final_biomass_g_L': float(results['final_biomass']),\n",
    "    'total_anomalies': int(results['num_anomalies']),\n",
    "    'success': bool(results['final_titer'] > 5.0),\n",
    "    'timestamp': datetime.now()}])\n",
    "\n",
    "summary_export.to_csv(output_path, index=False)\n",
    "print(f\"Summary exported to: {output_path}\")\n",
    "\n",
    "# ++ telemetry data\n",
    "telemetry_path = f\"/tmp/biopilot_run_{results['run_id']}_telemetry.csv\"\n",
    "results['observed_history'].to_csv(telemetry_path, index=False)\n",
    "print(f\"Telemetry exported to: {telemetry_path}\")\n",
    "\n",
    "# cleanup...\n",
    "print(f\"\\nrun ID: {results['run_id']}\")\n",
    "print(f\"Final titer: {results['final_titer']:.2f} mg/mL\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "synthetic_bioreactor.ipynb",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
