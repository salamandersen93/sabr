{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e5cc4a2-443e-4563-b10f-82be25ce25bb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# Enables autoreload; learn more at https://docs.databricks.com/en/files/workspace-modules.html#autoreload-for-python-modules\n",
    "# To disable autoreload; run %autoreload 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ae89453-fe3f-40c2-b215-7fdf8442d76b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import sys\n",
    "import os\n",
    "from typing import Dict\n",
    "\n",
    "# Set project paths\n",
    "PROJECT_ROOT = '/Workspace/Repos/synthetic-twin/synthetic_twin'\n",
    "MODULES_PATH = os.path.join(PROJECT_ROOT, 'modules')\n",
    "if MODULES_PATH not in sys.path:\n",
    "    sys.path.insert(0, MODULES_PATH)\n",
    "\n",
    "# Import BioPilot components\n",
    "from config import (\n",
    "    SIMULATION_PARAMS, INITIAL_STATE, KINETIC_PARAMS, \n",
    "    REACTOR_PARAMS, SENSOR_PARAMS, FAULT_TEMPLATES,\n",
    "    SCENARIOS, SCORING_CONFIG\n",
    ")\n",
    "from models import BioreactorSimulation, FaultManager\n",
    "from anomaly_detection import (\n",
    "    AnomalyDetectionEngine, \n",
    "    create_default_bioreactor_config\n",
    ")\n",
    "from agent_copilot import (\n",
    "    MultiAgentCopilot, \n",
    "    create_default_copilot_config,\n",
    "    AgentObservation\n",
    ")\n",
    "from data_lake import BioreactorDataLake\n",
    "from run_simulation_workflow import BioPilotWorkflow, visualize_run\n",
    "\n",
    "print(\"âœ… All modules imported successfully!\")\n",
    "print(f\"ðŸ“ Project root: {PROJECT_ROOT}\")\n",
    "\n",
    "# Assemble configuration\n",
    "config = {\n",
    "    'SIMULATION_PARAMS': SIMULATION_PARAMS,\n",
    "    'INITIAL_STATE': INITIAL_STATE,\n",
    "    'KINETIC_PARAMS': KINETIC_PARAMS,\n",
    "    'REACTOR_PARAMS': REACTOR_PARAMS,\n",
    "    'SENSOR_PARAMS': SENSOR_PARAMS,\n",
    "    'FAULT_TEMPLATES': FAULT_TEMPLATES\n",
    "}\n",
    "\n",
    "# Initialize workflow\n",
    "workflow = BioPilotWorkflow(\n",
    "    spark=spark,\n",
    "    config_dict=config,\n",
    "    enable_agent=True,\n",
    "    enable_anomaly_detection=True\n",
    ")\n",
    "\n",
    "# Inject a fault scenario\n",
    "print(\"ðŸ”§ Injecting 'overfeed' fault at t=20h...\")\n",
    "workflow.inject_scenario_faults(scenario=\"overfeed\")\n",
    "\n",
    "# Run simulation\n",
    "print(\"\\nðŸš€ Running simulation...\")\n",
    "results = workflow.run_with_monitoring(\n",
    "    base_feed_rate=0.1,\n",
    "    save_to_lake=True\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ… Simulation complete!\")\n",
    "print(f\"   Run ID: {results['run_id']}\")\n",
    "print(f\"   Final Titer: {results['final_titer']:.2f} mg/mL\")\n",
    "print(f\"   Anomalies Detected: {results['num_anomalies']}\")\n",
    "print(f\"   Agent Actions: {results['num_actions']}\")\n",
    "\n",
    "# Create comprehensive visualization\n",
    "visualize_run(results)\n",
    "\n",
    "# Display run report\n",
    "if results['agent_report']:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"AGENT COPILOT REPORT\")\n",
    "    print(\"=\"*60)\n",
    "    print(results['agent_report'])\n",
    "# Query telemetry data\n",
    "run_id = results['run_id']\n",
    "telemetry_df = workflow.data_lake.get_run_telemetry(spark, run_id, is_observed=True)\n",
    "\n",
    "print(f\"ðŸ“Š Telemetry Data Shape: {telemetry_df.shape}\")\n",
    "print(f\"\\nSignals captured: {telemetry_df['signal_name'].unique()}\")\n",
    "print(f\"Time range: {telemetry_df['time_h'].min():.1f} - {telemetry_df['time_h'].max():.1f} hours\")\n",
    "\n",
    "# Show sample\n",
    "display(telemetry_df.head(10))\n",
    "\n",
    "# Query anomaly scores\n",
    "anomalies_df = workflow.data_lake.get_anomalies(spark, run_id, only_detected=True)\n",
    "\n",
    "print(f\"ðŸš¨ Anomalies Detected: {len(anomalies_df)}\")\n",
    "\n",
    "if not anomalies_df.empty:\n",
    "    print(\"\\nAnomalies by signal:\")\n",
    "    print(anomalies_df.groupby('signal_name').size())\n",
    "    \n",
    "    print(\"\\nAnomalies by method:\")\n",
    "    print(anomalies_df.groupby('method').size())\n",
    "    \n",
    "    display(anomalies_df.head(10))\n",
    "else:\n",
    "    print(\"âœ… No anomalies detected in this run\")\n",
    "\n",
    "# Query agent actions\n",
    "actions_df = workflow.data_lake.get_agent_actions(spark, run_id)\n",
    "\n",
    "print(f\"ðŸ¤– Agent Actions Taken: {len(actions_df)}\")\n",
    "\n",
    "if not actions_df.empty:\n",
    "    print(\"\\nActions by type:\")\n",
    "    print(actions_df.groupby('action_type').size())\n",
    "    \n",
    "    display(actions_df)\n",
    "else:\n",
    "    print(\"â„¹ï¸ No agent actions were taken\")\n",
    "\n",
    "\n",
    "from run_simulation_workflow import run_batch_scenarios\n",
    "\n",
    "# Test multiple scenarios with replicates\n",
    "scenarios_to_test = ['baseline', 'overfeed', 'DO_drop', 'contamination']\n",
    "\n",
    "print(\"ðŸ”¬ Running batch scenario analysis...\")\n",
    "print(f\"   Scenarios: {scenarios_to_test}\")\n",
    "print(f\"   Replicates per scenario: 3\")\n",
    "print(f\"   Total runs: {len(scenarios_to_test) * 3}\")\n",
    "print(\"\\nThis may take several minutes...\\n\")\n",
    "\n",
    "batch_summary = run_batch_scenarios(\n",
    "    spark=spark,\n",
    "    scenarios=scenarios_to_test,\n",
    "    num_replicates=3\n",
    ")\n",
    "\n",
    "print(\"\\nâœ… Batch analysis complete!\")\n",
    "display(batch_summary)\n",
    "\n",
    "# Statistical summary by scenario\n",
    "summary_stats = batch_summary.groupby('scenario').agg({\n",
    "    'final_titer': ['mean', 'std', 'min', 'max'],\n",
    "    'final_biomass': ['mean', 'std'],\n",
    "    'num_anomalies': 'mean',\n",
    "    'num_actions': 'mean'\n",
    "}).round(3)\n",
    "\n",
    "print(\"\\nðŸ“ˆ Summary Statistics by Scenario:\")\n",
    "display(summary_stats)\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "# Fetch data for comparison\n",
    "comparison_query = \"\"\"\n",
    "SELECT \n",
    "    rm.scenario,\n",
    "    rm.final_titer,\n",
    "    rm.final_biomass,\n",
    "    rm.num_anomalies,\n",
    "    rm.num_actions,\n",
    "    rm.success,\n",
    "    rm.score\n",
    "FROM main.biopilot.run_metadata rm\n",
    "WHERE rm.scenario IN ('baseline', 'overfeed', 'DO_drop', 'contamination')\n",
    "\"\"\"\n",
    "\n",
    "comparison_df = spark.sql(comparison_query).toPandas()\n",
    "\n",
    "# Create comparison plots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Final titer by scenario\n",
    "sns.boxplot(data=comparison_df, x='scenario', y='final_titer', ax=axes[0, 0])\n",
    "axes[0, 0].set_title('Final Titer by Scenario', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].set_ylabel('Titer [mg/mL]')\n",
    "axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Anomalies by scenario\n",
    "sns.boxplot(data=comparison_df, x='scenario', y='num_anomalies', ax=axes[0, 1])\n",
    "axes[0, 1].set_title('Anomalies Detected by Scenario', fontsize=12, fontweight='bold')\n",
    "axes[0, 1].set_ylabel('Number of Anomalies')\n",
    "axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Agent actions by scenario\n",
    "sns.boxplot(data=comparison_df, x='scenario', y='num_actions', ax=axes[1, 0])\n",
    "axes[1, 0].set_title('Agent Actions by Scenario', fontsize=12, fontweight='bold')\n",
    "axes[1, 0].set_ylabel('Number of Actions')\n",
    "axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Success rate by scenario\n",
    "success_rate = comparison_df.groupby('scenario')['success'].mean() * 100\n",
    "success_rate.plot(kind='bar', ax=axes[1, 1], color='skyblue')\n",
    "axes[1, 1].set_title('Success Rate by Scenario', fontsize=12, fontweight='bold')\n",
    "axes[1, 1].set_ylabel('Success Rate [%]')\n",
    "axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "axes[1, 1].set_ylim([0, 100])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## 8. Custom Experiment - Parameter Sweep\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Test sensitivity to different growth rates (mu_max)\n",
    "mu_max_values = [0.03, 0.04, 0.05, 0.06]\n",
    "sweep_results = []\n",
    "\n",
    "print(\"ðŸ”¬ Running parameter sweep: mu_max\")\n",
    "print(f\"   Testing values: {mu_max_values}\")\n",
    "\n",
    "for mu_max in mu_max_values:\n",
    "    print(f\"\\n   Testing mu_max = {mu_max}...\")\n",
    "    \n",
    "    # Modify config\n",
    "    custom_config = config.copy()\n",
    "    custom_config['KINETIC_PARAMS'] = KINETIC_PARAMS.copy()\n",
    "    custom_config['KINETIC_PARAMS']['mu_max'] = mu_max\n",
    "    \n",
    "    # Run simulation\n",
    "    workflow_sweep = BioPilotWorkflow(\n",
    "        spark=spark,\n",
    "        config_dict=custom_config,\n",
    "        enable_agent=False,  # Disable for speed\n",
    "        enable_anomaly_detection=False\n",
    "    )\n",
    "    \n",
    "    results_sweep = workflow_sweep.run_with_monitoring(\n",
    "        base_feed_rate=0.1,\n",
    "        save_to_lake=True\n",
    "    )\n",
    "    \n",
    "    sweep_results.append({\n",
    "        'mu_max': mu_max,\n",
    "        'final_titer': results_sweep['final_titer'],\n",
    "        'final_biomass': results_sweep['final_biomass']\n",
    "    })\n",
    "\n",
    "# Analyze sweep results\n",
    "sweep_df = pd.DataFrame(sweep_results)\n",
    "print(\"\\nâœ… Parameter sweep complete!\")\n",
    "display(sweep_df)\n",
    "\n",
    "# Plot results\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "ax1.plot(sweep_df['mu_max'], sweep_df['final_titer'], 'o-', linewidth=2, markersize=8)\n",
    "ax1.set_xlabel('mu_max [1/h]', fontsize=11)\n",
    "ax1.set_ylabel('Final Titer [mg/mL]', fontsize=11)\n",
    "ax1.set_title('Titer vs Growth Rate', fontweight='bold')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "ax2.plot(sweep_df['mu_max'], sweep_df['final_biomass'], 'o-', \n",
    "         linewidth=2, markersize=8, color='green')\n",
    "ax2.set_xlabel('mu_max [1/h]', fontsize=11)\n",
    "ax2.set_ylabel('Final Biomass [g/L]', fontsize=11)\n",
    "ax2.set_title('Biomass vs Growth Rate', fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## 9. Export Results for Reporting\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Get comprehensive run summary\n",
    "run_summary = workflow.data_lake.get_run_summary(spark, results['run_id'])\n",
    "\n",
    "# Save summary to file\n",
    "output_path = f\"/tmp/biopilot_run_{results['run_id']}_summary.csv\"\n",
    "\n",
    "# Create summary DataFrame with native Python types\n",
    "summary_export = pd.DataFrame([{\n",
    "    'run_id': str(results['run_id']),\n",
    "    'scenario': 'overfeed',\n",
    "    'final_titer_mg_mL': float(results['final_titer']),\n",
    "    'final_biomass_g_L': float(results['final_biomass']),\n",
    "    'total_anomalies': int(results['num_anomalies']),\n",
    "    'total_actions': int(results['num_actions']),\n",
    "    'success': bool(results['final_titer'] > 5.0),\n",
    "    'timestamp': datetime.now()\n",
    "}])\n",
    "\n",
    "summary_export.to_csv(output_path, index=False)\n",
    "print(f\"âœ… Summary exported to: {output_path}\")\n",
    "\n",
    "# Also save telemetry\n",
    "telemetry_path = f\"/tmp/biopilot_run_{results['run_id']}_telemetry.csv\"\n",
    "results['observed_history'].to_csv(telemetry_path, index=False)\n",
    "print(f\"âœ… Telemetry exported to: {telemetry_path}\")\n",
    "\n",
    "# Final cleanup\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ðŸŽ‰ QUICKSTART COMPLETE!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nYour run ID: {results['run_id']}\")\n",
    "print(f\"Final titer: {results['final_titer']:.2f} mg/mL\")\n",
    "print(f\"Data saved to: main.biopilot.*\")\n",
    "print(\"\\nðŸ‘‰ Try modifying scenarios and running your own experiments!\")\n",
    "print(\"=\"*60)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "synthetic_bioreactor.ipynb",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
