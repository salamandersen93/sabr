{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e5cc4a2-443e-4563-b10f-82be25ce25bb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# Enables autoreload; learn more at https://docs.databricks.com/en/files/workspace-modules.html#autoreload-for-python-modules\n",
    "# To disable autoreload; run %autoreload 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ae89453-fe3f-40c2-b215-7fdf8442d76b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import sys\n",
    "import os\n",
    "from typing import Dict\n",
    "import seaborn as sns\n",
    "\n",
    "# setup\n",
    "PROJECT_ROOT = '/Workspace/Repos/synthetic-twin/synthetic_twin'\n",
    "MODULES_PATH = os.path.join(PROJECT_ROOT, 'modules')\n",
    "if MODULES_PATH not in sys.path:\n",
    "    sys.path.insert(0, MODULES_PATH)\n",
    "\n",
    "# import modules\n",
    "from config import (SIMULATION_PARAMS, INITIAL_STATE, KINETIC_PARAMS, \n",
    "                    REACTOR_PARAMS, SENSOR_PARAMS, FAULT_TEMPLATES, SCENARIOS, SCORING_CONFIG)\n",
    "\n",
    "from models import BioreactorSimulation, FaultManager\n",
    "from anomaly_detection import (AnomalyDetectionEngine, create_default_bioreactor_config)\n",
    "from agent_copilot import (MultiAgentCopilot, create_default_copilot_config, AgentObservation)\n",
    "from data_lake import BioreactorDataLake\n",
    "from run_simulation_workflow import BioPilotWorkflow, visualize_run, run_batch_scenarios\n",
    "print(\"All modules imported successfully!\")\n",
    "\n",
    "# configuration\n",
    "config = {\n",
    "    'SIMULATION_PARAMS': SIMULATION_PARAMS,\n",
    "    'INITIAL_STATE': INITIAL_STATE,\n",
    "    'KINETIC_PARAMS': KINETIC_PARAMS,\n",
    "    'REACTOR_PARAMS': REACTOR_PARAMS,\n",
    "    'SENSOR_PARAMS': SENSOR_PARAMS,\n",
    "    'FAULT_TEMPLATES': FAULT_TEMPLATES\n",
    "    }\n",
    "\n",
    "# initialize workflow\n",
    "workflow = BioPilotWorkflow(spark=spark,config_dict=config,\n",
    "                            enable_agent=True,enable_anomaly_detection=True)\n",
    "\n",
    "# injection of a fault scenario example - overfeeding @ 20h\n",
    "# TODO: make these random, or configurable based on user input\n",
    "print(\"Injecting 'overfeed' fault at t=20h...\")\n",
    "workflow.inject_scenario_faults(scenario=\"overfeed\")\n",
    "\n",
    "# initiate simulation with a default base feed rate\n",
    "# TODO: make base feed rate customizable/dynamic\n",
    "print(\"\\nRunning simulation...\")\n",
    "results = workflow.run_with_monitoring(base_feed_rate=0.1,save_to_lake=True)\n",
    "\n",
    "print(f\"\\nSimulation complete!\")\n",
    "print(f\"   Run ID: {results['run_id']}\")\n",
    "print(f\"   Final Titer: {results['final_titer']:.2f} mg/mL\")\n",
    "print(f\"   Anomalies Detected: {results['num_anomalies']}\")\n",
    "print(f\"   Agent Actions: {results['num_actions']}\")\n",
    "\n",
    "# output simulation visualizations\n",
    "visualize_run(results)\n",
    "\n",
    "# generate report for simulation run\n",
    "if results['agent_report']:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"AGENT COPILOT REPORT\")\n",
    "    print(\"=\"*60)\n",
    "    print(results['agent_report'])\n",
    "\n",
    "# query telemetry data (temporal data from simulation log)\n",
    "run_id = results['run_id']\n",
    "telemetry_df = workflow.data_lake.get_run_telemetry(spark, run_id, is_observed=True)\n",
    "\n",
    "print(f\"Telemetry Data Shape: {telemetry_df.shape}\")\n",
    "print(f\"\\nSignals captured: {telemetry_df['signal_name'].unique()}\")\n",
    "print(f\"Time range: {telemetry_df['time_h'].min():.1f} - {telemetry_df['time_h'].max():.1f} hours\")\n",
    "\n",
    "# telemetry sample...\n",
    "display(telemetry_df.head(10))\n",
    "\n",
    "# query anomaly scores (detected based on telemetry data using multiple methods)\n",
    "# TODO: anomalies are exteremely common, something is definitely wrong with data synthesis. Troubleshoot this.\n",
    "anomalies_df = workflow.data_lake.get_anomalies(spark, run_id, only_detected=True)\n",
    "\n",
    "print(f\"Anomalies Detected: {len(anomalies_df)}\")\n",
    "\n",
    "if not anomalies_df.empty:\n",
    "    print(\"\\nAnomalies by signal:\")\n",
    "    print(anomalies_df.groupby('signal_name').size())\n",
    "    \n",
    "    print(\"\\nAnomalies by method:\")\n",
    "    print(anomalies_df.groupby('method').size())\n",
    "    \n",
    "    display(anomalies_df.head(10))\n",
    "else:\n",
    "    print(\"No anomalies detected in this run\")\n",
    "\n",
    "# query agent actions\n",
    "actions_df = workflow.data_lake.get_agent_actions(spark, run_id)\n",
    "\n",
    "print(f\"Agent Actions Taken: {len(actions_df)}\")\n",
    "\n",
    "if not actions_df.empty:\n",
    "    print(\"\\nActions by type:\")\n",
    "    print(actions_df.groupby('action_type').size())\n",
    "    \n",
    "    display(actions_df)\n",
    "else:\n",
    "    print(\"No agent actions were taken\")\n",
    "\n",
    "# test multiple scenarios with replicates\n",
    "scenarios_to_test = ['standard', 'overfeed', 'DO_drop', 'contamination']\n",
    "\n",
    "print(\"Running batch scenario analysis...\")\n",
    "print(f\"   Scenarios: {scenarios_to_test}\")\n",
    "print(f\"   Replicates per scenario: 3\")\n",
    "print(f\"   Total runs: {len(scenarios_to_test) * 3}\")\n",
    "print(\"\\nThis may take several minutes...\\n\")\n",
    "\n",
    "batch_summary = run_batch_scenarios(spark=spark,scenarios=scenarios_to_test,num_replicates=3)\n",
    "\n",
    "print(\"\\nBatch analysis complete!\")\n",
    "display(batch_summary)\n",
    "\n",
    "# statistical summary by scenario\n",
    "summary_stats = batch_summary.groupby('scenario').agg({\n",
    "    'final_titer': ['mean', 'std', 'min', 'max'],\n",
    "    'final_biomass': ['mean', 'std'],\n",
    "    'num_anomalies': 'mean',\n",
    "    'num_actions': 'mean'}).round(3)\n",
    "\n",
    "print(\"\\nSummary Statistics by Scenario:\")\n",
    "display(summary_stats)\n",
    "\n",
    "# Fetch data for comparison\n",
    "comparison_query = \"\"\"\n",
    "        SELECT \n",
    "            rm.scenario,\n",
    "            rm.final_titer,\n",
    "            rm.num_anomalies,\n",
    "            rm.num_actions,\n",
    "            rm.success,\n",
    "            rm.score\n",
    "        FROM workspace.biopilot.run_metadata rm\n",
    "        WHERE rm.scenario IN ('standard', 'overfeed', 'DO_drop', 'contamination')\n",
    "        \"\"\"\n",
    "\n",
    "comparison_df = spark.sql(comparison_query).toPandas()\n",
    "\n",
    "# plot standard vs scenarios\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# final titer by scenario\n",
    "sns.boxplot(data=comparison_df, x='scenario', y='final_titer', ax=axes[0, 0])\n",
    "axes[0, 0].set_title('Final Titer by Scenario', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].set_ylabel('Titer [mg/mL]')\n",
    "axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# anomalies by scenario\n",
    "sns.boxplot(data=comparison_df, x='scenario', y='num_anomalies', ax=axes[0, 1])\n",
    "axes[0, 1].set_title('Anomalies Detected by Scenario', fontsize=12, fontweight='bold')\n",
    "axes[0, 1].set_ylabel('Number of Anomalies')\n",
    "axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# agent actions by scenario\n",
    "sns.boxplot(data=comparison_df, x='scenario', y='num_actions', ax=axes[1, 0])\n",
    "axes[1, 0].set_title('Agent Actions by Scenario', fontsize=12, fontweight='bold')\n",
    "axes[1, 0].set_ylabel('Number of Actions')\n",
    "axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# success rate by scenario\n",
    "success_rate = comparison_df.groupby('scenario')['success'].mean() * 100\n",
    "success_rate.plot(kind='bar', ax=axes[1, 1], color='skyblue')\n",
    "axes[1, 1].set_title('Success Rate by Scenario', fontsize=12, fontweight='bold')\n",
    "axes[1, 1].set_ylabel('Success Rate [%]')\n",
    "axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "axes[1, 1].set_ylim([0, 100])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# test sensitivity to different growth rates (mu_max) (parameter sweep)\n",
    "mu_max_values = [0.03, 0.04, 0.05, 0.06]\n",
    "sweep_results = []\n",
    "\n",
    "print(\"Running parameter sweep: mu_max\")\n",
    "print(f\"   Testing values: {mu_max_values}\")\n",
    "\n",
    "for mu_max in mu_max_values:\n",
    "    print(f\"\\n   Testing mu_max = {mu_max}...\")\n",
    "    \n",
    "    # modify config\n",
    "    custom_config = config.copy()\n",
    "    custom_config['KINETIC_PARAMS'] = KINETIC_PARAMS.copy()\n",
    "    custom_config['KINETIC_PARAMS']['mu_max'] = mu_max\n",
    "    \n",
    "    # run simulation\n",
    "    workflow_sweep = BioPilotWorkflow(spark=spark,config_dict=custom_config,\n",
    "                                      enable_agent=False,enable_anomaly_detection=False)\n",
    "    \n",
    "    results_sweep = workflow_sweep.run_with_monitoring(base_feed_rate=0.1,save_to_lake=Tru)\n",
    "    \n",
    "    sweep_results.append({'mu_max': mu_max,'final_titer': results_sweep['final_titer'],\n",
    "                          'final_biomass': results_sweep['final_biomass']})\n",
    "\n",
    "# analyze sweep results\n",
    "sweep_df = pd.DataFrame(sweep_results)\n",
    "print(\"\\nParameter sweep complete...\")\n",
    "display(sweep_df)\n",
    "\n",
    "# output sweep result visualizations\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "ax1.plot(sweep_df['mu_max'], sweep_df['final_titer'], 'o-', linewidth=2, markersize=8)\n",
    "ax1.set_xlabel('mu_max [1/h]', fontsize=11)\n",
    "ax1.set_ylabel('Final Titer [mg/mL]', fontsize=11)\n",
    "ax1.set_title('Titer vs Growth Rate', fontweight='bold')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "ax2.plot(sweep_df['mu_max'], sweep_df['final_biomass'], 'o-', \n",
    "         linewidth=2, markersize=8, color='green')\n",
    "ax2.set_xlabel('mu_max [1/h]', fontsize=11)\n",
    "ax2.set_ylabel('Final Biomass [g/L]', fontsize=11)\n",
    "ax2.set_title('Biomass vs Growth Rate', fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# generate run summary\n",
    "run_summary = workflow.data_lake.get_run_summary(spark, results['run_id'])\n",
    "output_path = f\"/tmp/biopilot_run_{results['run_id']}_summary.csv\"\n",
    "\n",
    "summary_export = pd.DataFrame([{\n",
    "    'run_id': str(results['run_id']),\n",
    "    'scenario': 'overfeed',\n",
    "    'final_titer_mg_mL': float(results['final_titer']),\n",
    "    'final_biomass_g_L': float(results['final_biomass']),\n",
    "    'total_anomalies': int(results['num_anomalies']),\n",
    "    'total_actions': int(results['num_actions']),\n",
    "    'success': bool(results['final_titer'] > 5.0),\n",
    "    'timestamp': datetime.now()}])\n",
    "\n",
    "summary_export.to_csv(output_path, index=False)\n",
    "print(f\"Summary exported to: {output_path}\")\n",
    "\n",
    "# ++ telemetry data\n",
    "telemetry_path = f\"/tmp/biopilot_run_{results['run_id']}_telemetry.csv\"\n",
    "results['observed_history'].to_csv(telemetry_path, index=False)\n",
    "print(f\"Telemetry exported to: {telemetry_path}\")\n",
    "\n",
    "# cleanup...\n",
    "print(f\"\\nrun ID: {results['run_id']}\")\n",
    "print(f\"Final titer: {results['final_titer']:.2f} mg/mL\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "synthetic_bioreactor.ipynb",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
